{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benjitaduratames/labo2025v/blob/main/src/rpart/301_OptimizacionHiperparametros_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DrvzQ9BsXmX"
      },
      "source": [
        "# 3 Optimización de Hiperpárametros"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.01 Introduccion\n",
        "En general los algoritmos que generan modelos predictivos poseen hiperparámetros que *dado un dataset* deben ser optimizados.\n",
        "<br> La invocación de uno de esos algoritmos sin hiperparámetros no es más que\n",
        "invocarlos con hiperparámetros por default definidos por el fabricante. Por ejemplo en el caso de la librería **rpart** es cp=0.01, maxdepth=30, minsplit=20, minbucket=6,  lo que en nuestro dataset genera un arbol de un solo nodo (decimos \"no se abre el arbol\"); la razon de esto es la proporcion de \"BAJA+1\" y \"BAJA+2\""
      ],
      "metadata": {
        "id": "ChXVnCOOuqOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el primer notebook de la asignatura usted probó optimizar manualmente los hiperparámetros entrenando en un mes completo y viendo los resultados directamente en el Public Leaderboard, que es una porción de los datos de futuro.\n",
        "<br> En el mundo real no se dispone jamás de la clase del futuro, con lo cual lo anterior es meramente un artifical divertimento.\n",
        "<br> La solución es estimar la bondad de un set de hiperparámetros en alguna combinación de:\n",
        "  * Una sola partición de  <training, testing>\n",
        "  * Multiples particiones de <training, testing>\n",
        "  * El método de  k-fold Cross Validation , generalmente con n>=5\n",
        "  * Utilizar   n-repated  k-fold Cross Validation\n",
        "  * Leave One Out  si la cardinalidad del dataset y el poder de cómputo se lo permiten"
      ],
      "metadata": {
        "id": "kXI3gt3iwNzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de comenzar a trabajar con el método de  \"Multiples particiones de <training, testing>  se le invitó a extender un esqueleto de código del método de **Optimización de Hiperparámetros por Grid Search**"
      ],
      "metadata": {
        "id": "Max6K39SxSRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.02 Conceptos\n",
        "\n",
        "En esta entrega veremos los siguiente conceptos:\n",
        "* El origen del overfitting en un arbol de decisión\n",
        "* *La maldición del ganador*, overfitting en los hiperparámetros ganadores, Selective Inference\n",
        "* Data Drifting\n",
        "* Alterntivas de búsqueda\n",
        "  * Grid Search, la fuerza bruta\n",
        "  * Bayesian Optimization, la heurística"
      ],
      "metadata": {
        "id": "V0uWfedQ0CgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.03  Corrida notebook inicial\n",
        "\n",
        "En el repositorio oficial de la asignatura se encuentra el notebook ./src/rpart/z102_FinalTrain.ipynb  que automaticamente hace el submit a la Competencia Analista Sr  de Kaggle.\n",
        "<br>  Ingrese a un nuevo Google Colab  y pruebe algunas corridas del notebook cambiando los hiperparámetros de rpart"
      ],
      "metadata": {
        "id": "xcdjPTy0_7ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.04 Origen del Overfitting en un arbol de decisión\n",
        "¿Qué combinacion de hiperparámetros overfitea un árbol de decisión, para nuestro dataset?\n",
        "<br>¿Cómo se ve el overfitting desde el punto de vista de las curvas de ganancia?"
      ],
      "metadata": {
        "id": "Cv_td2pLawzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este capítulo es que usted juegue manualmente con los hiperparámetros de un rpart, observe las curvas de ganancia generadas en una particion <training=50%, testing=50%>  y obtengla conclusiones sobre el fenómeno observado."
      ],
      "metadata": {
        "id": "vyO0nseY5e6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducimos el concepto de **Curva de Ganancia**\n",
        "<br> Al aplicar un modelo a un dataset se le asigna a cada registro una probabilidad, a su vez cada registro contribuye con una ganancia la que puede ser una pérdida o una ganancia.  \n",
        "<br>Ordenamos el dataset por probabilidad *descendente* y computamos la ganancia acumulada, generando de esta forma la curva de ganancia\n",
        "<br> Para visualizar el efecto del under/over  fitting adecuadamente, realizamos una particion  <training= 50%, testing= 50%>\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyo3JVGn24zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tener presente:\n",
        "<br> Overfitting  **NO**  es la diferencia entre las curvas\n",
        "<br> Lo que divide el underfitting del overfitting al aumentar la complejidad del modelo es la complejidad donde se alcanza la métrica máxima."
      ],
      "metadata": {
        "id": "HXVdNC0R4FZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Qué debe hacer usted?\n",
        "Probar al menos estas combinaciones:\n",
        "* **Arbol crecimiento descontrolado**\n",
        "   * cp= -1\n",
        "   * maxdepth= 30\n",
        "   * minsplit= 2\n",
        "   * minbucket= 1\n",
        "* Arbol talla reducida\n",
        "   * cp= -1\n",
        "   * maxdepth= 3\n",
        "   * minsplit= 20000\n",
        "   * minbucket= 10000\n"
      ],
      "metadata": {
        "id": "a1p3BNOU7x_X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0ySYPfa7Zr"
      },
      "source": [
        "#### Seteo del ambiente en Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGY7H9xza7Zr"
      },
      "source": [
        "Esta parte se debe correr con el runtime en Python3\n",
        "<br>Ir al menu, Runtime -> Change Runtime Type -> Runtime type ->  **Python 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PupIBNba7Zr"
      },
      "source": [
        "Conectar la virtual machine donde esta corriendo Google Colab con el  Google Drive, para poder tener persistencia de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LpZCst5a7Zs"
      },
      "outputs": [],
      "source": [
        "# primero establecer el Runtime de Python 3\n",
        "from google.colab import drive\n",
        "drive.mount('/content/.drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC_F-wla7Zs"
      },
      "source": [
        "Para correr la siguiente celda es fundamental en Arranque en Frio haber copiado el archivo kaggle.json al Google Drive, en la carpeta indicada en el instructivo\n",
        "\n",
        "<br>los siguientes comando estan en shell script de Linux\n",
        "*   Crear las carpetas en el Google Drive\n",
        "*   \"instalar\" el archivo kaggle.json desde el Google Drive a la virtual machine para que pueda ser utilizado por la libreria  kaggle de Python\n",
        "*   Bajar el  **dataset_pequeno**  al  Google Drive  y tambien al disco local de la virtual machine que esta corriendo Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWLelftXa7Zt"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p \"/content/.drive/My Drive/labo1\"\n",
        "mkdir -p \"/content/buckets\"\n",
        "ln -s \"/content/.drive/My Drive/labo1\" /content/buckets/b1\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "cp /content/buckets/b1/kaggle/kaggle.json  ~/.kaggle\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "mkdir -p /content/buckets/b1/exp\n",
        "mkdir -p /content/buckets/b1/datasets\n",
        "mkdir -p /content/datasets\n",
        "\n",
        "\n",
        "\n",
        "archivo_origen=\"https://storage.googleapis.com/open-courses/austral2025-af91/dataset_pequeno.csv\"\n",
        "archivo_destino=\"/content/datasets/dataset_pequeno.csv\"\n",
        "archivo_destino_bucket=\"/content/buckets/b1/datasets/dataset_pequeno.csv\"\n",
        "\n",
        "if ! test -f $archivo_destino_bucket; then\n",
        "  wget  $archivo_origen  -O $archivo_destino_bucket\n",
        "fi\n",
        "\n",
        "\n",
        "if ! test -f $archivo_destino; then\n",
        "  cp  $archivo_destino_bucket  $archivo_destino\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxhSJ-oqb-r6"
      },
      "source": [
        "Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Type -> Runtime type -> R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls15rN6Ob-r7"
      },
      "source": [
        "limpio el ambiente de R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrRq-FePb-r7"
      },
      "outputs": [],
      "source": [
        "# limpio la memoria\n",
        "rm(list=ls(all.names=TRUE)) # remove all objects\n",
        "gc(full=TRUE, verbose=FALSE) # garbage collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIeRpVlZb-r8"
      },
      "outputs": [],
      "source": [
        "# cargo las librerias que necesito\n",
        "require(\"data.table\")\n",
        "require(\"rpart\")\n",
        "require(\"ggplot2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "experimento <- \"exp304\"\n",
        "dir.create(experimento, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", experimento ))"
      ],
      "metadata": {
        "id": "3Wb-iYIGb-r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acción :  Jugar con  minsplit, minbucket y maxdepth"
      ],
      "metadata": {
        "id": "DjLKeoBHhhNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cambiar aqui los parametros\n",
        "PARAM <- list()\n",
        "PARAM$semilla_primigenia <- 102191\n",
        "\n",
        "\n",
        "PARAM$minsplit <- 300\n",
        "PARAM$minbucket <- 20\n",
        "PARAM$maxdepth <- 11"
      ],
      "metadata": {
        "id": "b-kYnwWzcRHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# particionar agrega una columna llamada fold a un dataset\n",
        "#   que consiste en una particion estratificada segun agrupa\n",
        "# particionar( data=dataset, division=c(70,30),\n",
        "#  agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30\n",
        "\n",
        "particionar <- function(data, division, agrupa= \"\", campo= \"fold\", start= 1, seed= NA) {\n",
        "  if (!is.na(seed)) set.seed(seed)\n",
        "\n",
        "  bloque <- unlist(mapply(\n",
        "    function(x, y) {rep(y, x)},division, seq(from= start, length.out= length(division))))\n",
        "\n",
        "  data[, (campo) := sample(rep(bloque,ceiling(.N / length(bloque))))[1:.N],by= agrupa]\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZqrrKQ5xcSvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura del dataset\n",
        "\n",
        "dataset <- fread(\"/content/datasets/dataset_pequeno.csv\")"
      ],
      "metadata": {
        "id": "9pwX8GTacf3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a partir de ahora solo trabajo con 202107, el mes que tiene clase\n",
        "\n",
        "dataset <- dataset[foto_mes == 202107] # defino donde voy a entrenar"
      ],
      "metadata": {
        "id": "IaC-Ju2-ch-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La division training/testing es 50%, 50%\n",
        "#  que sea 50/50 se indica con el c(1,1)\n",
        "\n",
        "particionar(dataset,\n",
        "  division= c(1, 1),\n",
        "  agrupa= \"clase_ternaria\",\n",
        "  seed= PARAM$semilla_primigenia\n",
        ")"
      ],
      "metadata": {
        "id": "eKJeTb1icluu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entreno el modelo\n",
        "# los datos donde voy a entrenar\n",
        "# aqui es donde se deben probar distintos hiperparametros\n",
        "\n",
        "modelo <- rpart(\n",
        "  formula= \"clase_ternaria ~ . -fold\",\n",
        "  data= dataset[fold == 1, ],\n",
        "  xval= 0,\n",
        "  cp= -1,\n",
        "  minsplit= PARAM$minsplit,\n",
        "  minbucket= PARAM$minbucket,\n",
        "  maxdepth= PARAM$maxdepth\n",
        ")"
      ],
      "metadata": {
        "id": "v5Fuek3mc1Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aplico el modelo a TODOS los datos, inclusive los de training\n",
        "prediccion <- predict(modelo, dataset, type= \"prob\")"
      ],
      "metadata": {
        "id": "zjFQmTkFdGn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pego la probabilidad de  BAJA+2\n",
        "tb_prediccion <- dataset[, list(fold,clase_ternaria)]\n",
        "tb_prediccion[, prob_baja2 := prediccion[, \"BAJA+2\"]]"
      ],
      "metadata": {
        "id": "wxsRJ6-ldKMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dibujo la curva de ganancia acumulada\n",
        "setorder(tb_prediccion, fold, -prob_baja2)"
      ],
      "metadata": {
        "id": "CFPDDXxrdbHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agrego una columna que es la de las ganancias\n",
        "# la multiplico por 2 para que ya este normalizada\n",
        "#  es 2 porque cada fold es el 50%\n",
        "\n",
        "tb_prediccion[, gan := 2 *ifelse(clase_ternaria == \"BAJA+2\", 117000, -3000)]\n",
        "tb_prediccion[, ganancia_acumulada := cumsum(gan), by= fold]\n",
        "tb_prediccion[, pos := sequence(.N), by= fold]"
      ],
      "metadata": {
        "id": "bUPJACeXdoDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb_prediccion"
      ],
      "metadata": {
        "id": "mNdaJEmMe5gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agrego una columna que es la de las ganancias\n",
        "# la multiplico por 2 para que ya este normalizada\n",
        "#  es 2 porque cada fold es el 50%\n",
        "\n",
        "tb_prediccion[, gan := 2 *ifelse(clase_ternaria == \"BAJA+2\", 117000, -3000)]\n",
        "tb_prediccion[, ganancia_acumulada := cumsum(gan), by= fold]\n",
        "tb_prediccion[, pos := sequence(.N), by= fold]"
      ],
      "metadata": {
        "id": "687CCwPMfaDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defino hasta donde muestra el grafico\n",
        "amostrar <- 20000"
      ],
      "metadata": {
        "id": "Gnzpijujfi8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta hermosa curva muestra como en el mentiroso training\n",
        "#   la ganancia es siempre mejor que en el real testing\n",
        "\n",
        "options( repr.plot.width=10, repr.plot.height=10)\n",
        "\n",
        "gra <- ggplot(\n",
        "           data= tb_prediccion[pos <= amostrar],\n",
        "           aes( x= pos, y= ganancia_acumulada,\n",
        "                color= ifelse(fold == 1, \"train\", \"test\") )\n",
        "             ) + geom_line()\n",
        "\n",
        "print( gra )\n"
      ],
      "metadata": {
        "id": "YCTWdnn5c-T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# veo los resultados\n",
        "\n",
        "print(PARAM)\n",
        "cat( \"Train gan max: \", tb_prediccion[fold==1, max(ganancia_acumulada)], \"\\n\" )\n",
        "cat( \"Test  gan max: \", tb_prediccion[fold==2, max(ganancia_acumulada)], \"\\n\" )\n"
      ],
      "metadata": {
        "id": "pCXOZ8bVfnsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.05 Análisis de la salida de Grid Search\n",
        "\n",
        "En clase utilizando un enfoque constructivista de educacion cada uno de los grupos del aula analizará las salidas de las corridas de Grid Search de la Tarea para el Hogar.\n",
        "<br>Se espera que quienes ya trabajan como Data Analyst se luzcan en el análisis de esos datos\n",
        "<br>Finalmente se utilizara un *arma conceptual secreta*, iluminando elegantemente donde están las mayores ganancias.\n",
        "\n",
        "<br><br>Si usted no tuvo la oportunidad de hacer sus propias corridas esta generosa cátedra pone a su diposición esta salida https://storage.googleapis.com/open-courses/austral2025-af91/gridsearch.txt"
      ],
      "metadata": {
        "id": "BPZj2gpfvvSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.06 La Maldición del Ganador"
      ],
      "metadata": {
        "id": "xyHuVU4wK0Xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Los hiperparámetros ganadores de toda la cohorte, estan overfiteando?\n",
        "<br> Trabajaremos con la hoja **C3-GS Overfitting** de la Google Sheet Colaborativa\n",
        "<br> Determinar de toda la cohorte quien obtuvo la mayor ganancia y con qué hiperparámetros\n",
        "<br> copiarlos  a la hoja **C3-GS Overfitting**\n",
        "<br> Quien obtuvo esa ganancia no debe hacer nada\n",
        "<br> El resto de la cohorte, modifica su script de Grid Search para calcular la ganancia de dichos hiperparámetros ganadores  utilizando su Semilla Primigenia y registra la ganancia en la linea correspondiente a su nombre en la hoja C3-GS Overfitting"
      ],
      "metadata": {
        "id": "olG0m0HQK4Ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿ Como compara la ganancia del ganador versus el resto de las ganancias recién calculadas ?"
      ],
      "metadata": {
        "id": "ch9ilhfJMFhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bibliografia"
      ],
      "metadata": {
        "id": "SDTdFtJzPdCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Selective Inference - the silent killer of replicability   https://www.youtube.com/watch?v=6ZxIzVjV1DE\n",
        "* Ioannidis, J. P. A. Why most published research findings are false. PLoS Med. 2, e124 (2005). https://journals.plos.org/plosmedicine/article/file?id=10.1371/journal.pmed.0020124&type=printable"
      ],
      "metadata": {
        "id": "OtOkVKZ9PfY0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBl_eIeO7WuM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.07 Data Drifting  sospechas"
      ],
      "metadata": {
        "id": "QNSznolLMVyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordenar la salida del Grid Search en forma descendente por ganancia (en testing obviamente)\n",
        "<br> De esta forma la posición 1 corresponde a la mayor ganancia, la 2 a la segunda mejor, etc\n",
        "<br> En la Google Sheet Colaborativa,  hoja  **C3- GridSEarch** cargue las posiciones  1, 2, 5, 10, 50 y 100 de la salida del Grid Search, dejando la columna Public Leaderboard sin cargar\n",
        "<br> La columna ganancia_mean tiene valores en orden descendente"
      ],
      "metadata": {
        "id": "HMVNxSEcMdiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de hacer Grid Search  es utilizando particiones <training, testing>  encontrar los mejores hiperparámetros\n",
        "<br> Esto tiene sentido en la medida que los hiperparámetros que resultan mejores de la búsqueda Grid Search son también los mejores cuando se hace el Final Training"
      ],
      "metadata": {
        "id": "T0SNDZq7NdGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando el notebook de la primiera clase,  **z102_FinalTrain.ipynb**   calcule para cada uno de los sets de hiperparámetros de las posiciones 1, 2, 5, 10, 50 y 100  cual es la ganancia en el Public Leaderboard de Kaggle\n",
        "<br> Deberá hacer una corrida para cada conjunto de hiperparámetros"
      ],
      "metadata": {
        "id": "-4M7yP3VOAm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ¿ Se cumple que los hiperparámetros de la posición  1 del Grid Search son los que mejor funcionan para predecir los datos del futuro ?"
      ],
      "metadata": {
        "id": "aGcjHfd3Ofmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿ Si esto no fuera así, estamos en una sitacion de **Game Over** ?"
      ],
      "metadata": {
        "id": "vbAo1HByPEdy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc9x9DnsNlZv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.08 Data Drifting, breve intuicion"
      ],
      "metadata": {
        "id": "-ctmDt7dPRgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se mostrará un posible origen de las discrepancias observadas en el capítulo anterior\n",
        "<br> La solución al Data Drifting es otro precio ..."
      ],
      "metadata": {
        "id": "hMuubs9vQKYC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgGgjPQuGimP"
      },
      "source": [
        "Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Type -> Runtime type -> R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6AxqLwOGimQ"
      },
      "source": [
        "limpio el ambiente de R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797Ad0_QGimQ"
      },
      "outputs": [],
      "source": [
        "# limpio la memoria\n",
        "rm(list=ls(all.names=TRUE)) # remove all objects\n",
        "gc(full=TRUE, verbose=FALSE) # garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CepvbjGimQ"
      },
      "source": [
        "* Instalacion de la libreria  rpart.plot  para dibujar el arbol\n",
        "* invocacion de las librerias  **data.table** y  **rpart**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRkZ1V2_GimR"
      },
      "outputs": [],
      "source": [
        "# cargo las librerias que necesito\n",
        "require(\"data.table\")\n",
        "require(\"rpart\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "experimento <- \"exp308\"\n",
        "dir.create(experimento, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", experimento ))"
      ],
      "metadata": {
        "id": "NB9XL85QSREx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM <- list()\n",
        "PARAM$mes0 <- 202107\n",
        "PARAM$mes1 <- 202109"
      ],
      "metadata": {
        "id": "YWVW1EI3SXRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graficar_campo <- function(campo, param) {\n",
        "  # quito de grafico las colas del 5% de las densidades\n",
        "  qA <- quantile(dataset[foto_mes == param$mes0, get(campo)],\n",
        "    prob= c(0.05, 0.95), na.rm= TRUE\n",
        "  )\n",
        "\n",
        "  qB <- quantile(dataset[foto_mes == param$mes1, get(campo)],\n",
        "    prob= c(0.05, 0.95), na.rm= TRUE\n",
        "  )\n",
        "\n",
        "  xxmin <- pmin(qA[[1]], qB[[1]])\n",
        "  xxmax <- pmax(qA[[2]], qB[[2]])\n",
        "\n",
        "  densidad_A <- density(dataset[foto_mes == param$mes0, get(campo)],\n",
        "    kernel= \"gaussian\", na.rm= TRUE\n",
        "  )\n",
        "\n",
        "  densidad_B <- density(dataset[foto_mes == param$mes1, get(campo)],\n",
        "    kernel= \"gaussian\", na.rm= TRUE\n",
        "  )\n",
        "\n",
        "  plot(densidad_A,\n",
        "    col= \"blue\",\n",
        "    xlim= c(xxmin, xxmax),\n",
        "    ylim= c(0, pmax(max(densidad_A$y), max(densidad_B$y))),\n",
        "    main= campo\n",
        "  )\n",
        "\n",
        "  lines(densidad_B, col= \"red\", lty= 2)\n",
        "\n",
        "  legend(\"topright\",\n",
        "    legend= c( param$mes0, param$mes1),\n",
        "    col= c(\"blue\", \"red\"), lty= c(1, 2)\n",
        "  )\n",
        "}\n"
      ],
      "metadata": {
        "id": "o4Ec0SIXSnjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura del dataset\n",
        "dataset <- fread(\"/content/datasets/dataset_pequeno.csv\")"
      ],
      "metadata": {
        "id": "qa6PCztQTIZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entreno el modelo\n",
        "# utilizo los mejores hiperparametros encontrados\n",
        "\n",
        "modelo <- rpart(\n",
        "  formula= \"clase_ternaria ~ . \",\n",
        "  data= dataset[foto_mes == PARAM$mes0], # los datos donde voy a entrenar\n",
        "  xval= 0,\n",
        "  cp= -1,\n",
        "  minsplit= 1144,\n",
        "  minbucket= 539,\n",
        "  maxdepth= 8\n",
        ")\n"
      ],
      "metadata": {
        "id": "ijE1YZvXTYxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "campos_modelo <- names(modelo$variable.importance)\n",
        "campos_buenos <- c(campos_modelo, setdiff(colnames(dataset), campos_modelo))\n",
        "campos_buenos <- setdiff(\n",
        "  campos_buenos,\n",
        "  c(\"foto_mes\", \"clase_ternaria\")\n",
        ")\n",
        "\n",
        "campos_buenos"
      ],
      "metadata": {
        "id": "HS8Sj8aNUIJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para fines didacticos,  cliente_antiguedad primero\n",
        "campos_buenos <- c(\"cliente_antiguedad\", campos_buenos)"
      ],
      "metadata": {
        "id": "05cmWkKrVUAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grafico las densidades de cada variable para los dos mses\n",
        "\n",
        "options( repr.plot.width=15, repr.plot.height=15)\n",
        "\n",
        "for (campo in campos_buenos) {\n",
        "  cat(campo, \"  \")\n",
        "  graficar_campo(campo, PARAM)\n",
        "}\n"
      ],
      "metadata": {
        "id": "009BAOZ3UQV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCjhc7VGOySp"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.09 Bayesian Optimization"
      ],
      "metadata": {
        "id": "5cZrnAixV3gT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se explicará en clase como a partir del Grid Search se deriva el algoritmo de Bayesian Optimization"
      ],
      "metadata": {
        "id": "jRnL1RAGWTKy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMr6Z1enOyd3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.10 Bayesian Optimization código"
      ],
      "metadata": {
        "id": "khI14CLoWfCK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb0kVc82WnwU"
      },
      "source": [
        "Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Type -> Runtime type -> R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1yj6LSBWnwU"
      },
      "source": [
        "limpio el ambiente de R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THfA4LSjWnwU"
      },
      "outputs": [],
      "source": [
        "# limpio la memoria\n",
        "rm(list=ls(all.names=TRUE)) # remove all objects\n",
        "gc(full=TRUE, verbose=FALSE) # garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzI9HIJWnwV"
      },
      "source": [
        "* Instalacion de la libreria  rpart.plot  para dibujar el arbol\n",
        "* invocacion de las librerias  **data.table** y  **rpart**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeCTdZGyWnwV"
      },
      "outputs": [],
      "source": [
        "# cargo las librerias que necesito\n",
        "require(\"data.table\")\n",
        "require(\"rpart\")\n",
        "require(\"parallel\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if( ! require(\"rlist\") ) install.packages(\"rlist\")\n",
        "require(\"rlist\")"
      ],
      "metadata": {
        "id": "MjmheQDhkI9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paquete necesarios para la Bayesian Optimization\n",
        "if( !require(\"DiceKriging\") ) install.packages(\"DiceKriging\")\n",
        "require(\"DiceKriging\")"
      ],
      "metadata": {
        "id": "CzTjEz__kOrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paquete necesarios para la Bayesian Optimization\n",
        "if( !require(\"mlrMBO\") ) install.packages(\"mlrMBO\")\n",
        "require(\"mlrMBO\")"
      ],
      "metadata": {
        "id": "0irLrQDkkdPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accion a realizar : cambiar por su semilla primigenia"
      ],
      "metadata": {
        "id": "0h5tn-8QL1YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defino la  Optimizacion Bayesiana\n",
        "\n",
        "# cantidad de iteraciones de la Optimizacion Bayesiana\n",
        "PARAM <- list()\n",
        "\n",
        "PARAM$semilla_primigenia <- 102191\n",
        "PARAM$experimento <- \"HT310\"\n",
        "\n",
        "PARAM$BO_iter <- 40 #cantidad de iteraciones de la Bayesian Optimization\n",
        "\n",
        "# la letra L al final de 1L significa ENTERO\n",
        "PARAM$hs <- makeParamSet(\n",
        "    makeNumericParam(\"cp\", lower= -1, upper= 0.1),\n",
        "    makeIntegerParam(\"minsplit\", lower= 1L, upper= 8000L),\n",
        "    makeIntegerParam(\"minbucket\", lower= 1L, upper= 4000L),\n",
        "    makeIntegerParam(\"maxdepth\", lower= 3L, upper= 20L),\n",
        "    forbidden= quote(minbucket > 0.5 * minsplit)\n",
        ")\n",
        "# minbuket NO PUEDE ser mayor que la mitad de minsplit\n",
        "\n"
      ],
      "metadata": {
        "id": "rFmJWC1nlCaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "dir.create(PARAM$experimento, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", PARAM$experimento ))"
      ],
      "metadata": {
        "id": "xUuITlXAOp-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "particionar <- function(data, division, agrupa = \"\", campo = \"fold\",\n",
        "                        start = 1, seed = NA) {\n",
        "  if (!is.na(seed)) set.seed(seed)\n",
        "\n",
        "  bloque <- unlist(mapply(\n",
        "    function(x, y) {\n",
        "      rep(y, x)\n",
        "    }, division,\n",
        "    seq(from= start, length.out= length(division))\n",
        "  ))\n",
        "\n",
        "  data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],\n",
        "    by= agrupa\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "0AWMWd5dlHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fold_test  tiene el numero de fold que voy a usar para testear,\n",
        "#  entreno en el resto de los folds\n",
        "# param tiene los hiperparametros del arbol\n",
        "\n",
        "ArbolSimple <- function(fold_test, param_rpart) {\n",
        "  # genero el modelo\n",
        "  # entreno en todo MENOS el fold_test que uso para testing\n",
        "  modelo <- rpart(\"clase_ternaria ~ .\",\n",
        "    data= dataset[fold != fold_test, ],\n",
        "    xval= 0,\n",
        "    control= param_rpart\n",
        "  )\n",
        "\n",
        "  # aplico el modelo a los datos de testing\n",
        "  # aplico el modelo sobre los datos de testing\n",
        "  # quiero que me devuelva probabilidades\n",
        "  prediccion <- predict(modelo,\n",
        "    dataset[fold == fold_test, ],\n",
        "    type= \"prob\"\n",
        "  )\n",
        "\n",
        "  # esta es la probabilidad de baja\n",
        "  prob_baja2 <- prediccion[, \"BAJA+2\"]\n",
        "\n",
        "  # calculo la ganancia\n",
        "  ganancia_testing <- dataset[fold == fold_test][\n",
        "    prob_baja2 > 1 / 40,\n",
        "    sum(ifelse(clase_ternaria == \"BAJA+2\",\n",
        "      117000, -3000\n",
        "    ))\n",
        "  ]\n",
        "\n",
        "  # esta es la ganancia sobre el fold de testing, NO esta normalizada\n",
        "  return(ganancia_testing)\n",
        "}\n"
      ],
      "metadata": {
        "id": "Bij2dW0ElKZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ArbolesCrossValidation <- function(param_rpart, qfolds, pagrupa, semilla) {\n",
        "  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos\n",
        "  divi <- rep(1, qfolds)\n",
        "\n",
        "  # particiono en dataset en folds\n",
        "  particionar(dataset, divi, seed= semilla, agrupa= pagrupa)\n",
        "\n",
        "  ganancias <- mcmapply(ArbolSimple,\n",
        "    seq(qfolds), # 1 2 3 4 5\n",
        "    MoreArgs= list(param_rpart),\n",
        "    SIMPLIFY= FALSE,\n",
        "    mc.cores= detectCores()\n",
        "  )\n",
        "\n",
        "  dataset[, fold := NULL]\n",
        "\n",
        "  # devuelvo la primer ganancia y el promedio\n",
        "  # promedio las ganancias\n",
        "  ganancia_promedio <- mean(unlist(ganancias))\n",
        "  # aqui normalizo la ganancia\n",
        "  ganancia_promedio_normalizada <- ganancia_promedio * qfolds\n",
        "\n",
        "  return(ganancia_promedio_normalizada)\n",
        "}\n"
      ],
      "metadata": {
        "id": "1PoSYV0NlQq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esta funcion solo puede recibir los parametros que se estan optimizando\n",
        "# el resto de los parametros, lamentablemente se pasan como variables globales\n",
        "\n",
        "EstimarGanancia <- function(x) {\n",
        "  message(format(Sys.time(), \"%a %b %d %X %Y\"))\n",
        "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1\n",
        "\n",
        "  xval_folds <- 5\n",
        "  # param= x los hiperparametros del arbol\n",
        "  # qfolds= xval_folds  la cantidad de folds\n",
        "  ganancia <- ArbolesCrossValidation(\n",
        "    param_rpart= x,\n",
        "    qfolds= xval_folds,\n",
        "    pagrupa= \"clase_ternaria\",\n",
        "    semilla= PARAM$semilla_primigenia\n",
        "  )\n",
        "\n",
        "  return(ganancia)\n",
        "}\n"
      ],
      "metadata": {
        "id": "wWVKBCNilUxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura del dataset\n",
        "dataset <- fread(\"/content/datasets/dataset_pequeno.csv\")"
      ],
      "metadata": {
        "id": "IALv-toylgt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "archivo_log <- \"BO_log.txt\"\n",
        "archivo_BO <- \"bayesian.RDATA\"\n",
        "\n",
        "# leo si ya existe el log\n",
        "#  para retomar en caso que se se corte el programa\n",
        "GLOBAL_iteracion <- 0\n",
        "GLOBAL_mejor <- -Inf\n",
        "\n",
        "if (file.exists(archivo_log)) {\n",
        "  tabla_log <- fread(archivo_log)\n",
        "  GLOBAL_iteracion <- nrow(tabla_log)\n",
        "  GLOBAL_mejor <- tabla_log[, max(ganancia)]\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "l-0H-PQ9lzJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aqui comienza la configuracion de la Bayesian Optimization\n",
        "\n",
        "funcion_optimizar <- EstimarGanancia\n",
        "\n",
        "configureMlr(show.learner.output= FALSE)\n",
        "\n",
        "# configuro la busqueda bayesiana,\n",
        "#  los hiperparametros que se van a optimizar\n",
        "# por favor, no desesperarse por lo complejo\n",
        "# minimize= FALSE estoy Maximizando la ganancia\n",
        "obj.fun <- makeSingleObjectiveFunction(\n",
        "  fn= funcion_optimizar,\n",
        "  minimize= FALSE,\n",
        "  noisy= TRUE,\n",
        "  par.set= PARAM$hs,\n",
        "  has.simple.signature= FALSE\n",
        ")\n",
        "\n",
        "ctrl <- makeMBOControl(\n",
        "  save.on.disk.at.time= 600,\n",
        "  save.file.path= archivo_BO\n",
        ")\n",
        "\n",
        "ctrl <- setMBOControlTermination(ctrl, iters= PARAM$BO_iter)\n",
        "ctrl <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())\n",
        "\n",
        "surr.km <- makeLearner(\"regr.km\",\n",
        "  predict.type= \"se\",\n",
        "  covtype= \"matern3_2\", control= list(trace= TRUE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "HseppAV1l1bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inicio la optimizacion bayesiana\n",
        "if (!file.exists(archivo_BO)) {\n",
        "  bayesiana_salida <- mbo(\n",
        "    fun= obj.fun,\n",
        "    learner= surr.km,\n",
        "    control= ctrl\n",
        "  )\n",
        "} else {\n",
        "  bayesiana_salida <- mboContinue(archivo_BO)\n",
        "}\n",
        "# retomo en caso que ya exista\n"
      ],
      "metadata": {
        "id": "TAr8HSCLl5p7",
        "outputId": "f87107a1-5a8a-4d41-a789-0b8569f751b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing y column(s) for design. Not provided.\n",
            "\n",
            "Sat Oct 25 07:59:08 PM 2025\n",
            "\n",
            "Sat Oct 25 08:06:32 PM 2025\n",
            "\n",
            "Sat Oct 25 08:15:35 PM 2025\n",
            "\n",
            "Sat Oct 25 08:26:04 PM 2025\n",
            "\n",
            "Sat Oct 25 08:35:21 PM 2025\n",
            "\n",
            "Sat Oct 25 08:46:20 PM 2025\n",
            "\n",
            "Sat Oct 25 08:57:30 PM 2025\n",
            "\n",
            "Sat Oct 25 09:02:25 PM 2025\n",
            "\n",
            "Sat Oct 25 09:06:53 PM 2025\n",
            "\n",
            "Sat Oct 25 09:09:24 PM 2025\n",
            "\n",
            "Sat Oct 25 09:18:46 PM 2025\n",
            "\n",
            "Sat Oct 25 09:23:01 PM 2025\n",
            "\n",
            "Sat Oct 25 09:33:59 PM 2025\n",
            "\n",
            "Sat Oct 25 09:44:11 PM 2025\n",
            "\n",
            "Sat Oct 25 09:55:01 PM 2025\n",
            "\n",
            "Sat Oct 25 10:06:20 PM 2025\n",
            "\n",
            "[mbo] 0: cp=-0.24; minsplit=6686; minbucket=1480; maxdepth=7 : y = 4.85e+07 : 443.7 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.718; minsplit=4570; minbucket=2124; maxdepth=10 : y = 4.76e+07 : 543.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.442; minsplit=5962; minbucket=370; maxdepth=13 : y = 4.82e+07 : 629.2 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.117; minsplit=7672; minbucket=1698; maxdepth=16 : y = 4.8e+07 : 556.8 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.83; minsplit=2129; minbucket=1032; maxdepth=17 : y = 4.97e+07 : 659.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.284; minsplit=1355; minbucket=126; maxdepth=12 : y = 4.96e+07 : 669.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.354; minsplit=6246; minbucket=3056; maxdepth=4 : y = 4.77e+07 : 294.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.746; minsplit=3298; minbucket=663; maxdepth=3 : y = 3.72e+07 : 268.5 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=0.0458; minsplit=7096; minbucket=765; maxdepth=10 : y = 0 : 150.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.354; minsplit=6722; minbucket=489; maxdepth=11 : y = 4.87e+07 : 562.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.2; minsplit=7955; minbucket=2169; maxdepth=3 : y = 3.77e+07 : 255.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.425; minsplit=4903; minbucket=869; maxdepth=20 : y = 4.8e+07 : 658.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.586; minsplit=4406; minbucket=1239; maxdepth=14 : y = 4.8e+07 : 611.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.563; minsplit=5451; minbucket=847; maxdepth=15 : y = 4.82e+07 : 650.4 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.182; minsplit=2732; minbucket=912; maxdepth=20 : y = 4.72e+07 : 678.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: cp=-0.797; minsplit=6465; minbucket=1069; maxdepth=13 : y = 4.87e+07 : 602.9 secs : initdesign\n",
            "\n",
            "Saved the current state after iteration 1 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 10:16:25 PM 2025\n",
            "\n",
            "[mbo] 1: cp=-0.252; minsplit=6360; minbucket=1899; maxdepth=17 : y = 4.8e+07 : 558.0 secs : infill_ei\n",
            "\n",
            "Sat Oct 25 10:25:45 PM 2025\n",
            "\n",
            "[mbo] 2: cp=-0.316; minsplit=3450; minbucket=1388; maxdepth=8 : y = 4.71e+07 : 483.4 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 3 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 10:33:51 PM 2025\n",
            "\n",
            "[mbo] 3: cp=-0.817; minsplit=5011; minbucket=2387; maxdepth=20 : y = 4.78e+07 : 547.3 secs : infill_ei\n",
            "\n",
            "Sat Oct 25 10:43:00 PM 2025\n",
            "\n",
            "[mbo] 4: cp=-0.251; minsplit=5913; minbucket=7; maxdepth=15 : y = 4.75e+07 : 727.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 5 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 10:55:09 PM 2025\n",
            "\n",
            "[mbo] 5: cp=-0.496; minsplit=6724; minbucket=2762; maxdepth=12 : y = 4.8e+07 : 538.2 secs : infill_ei\n",
            "\n",
            "Sat Oct 25 11:04:10 PM 2025\n",
            "\n",
            "[mbo] 6: cp=-0.685; minsplit=6877; minbucket=1570; maxdepth=20 : y = 4.85e+07 : 553.1 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 7 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 11:13:26 PM 2025\n",
            "\n",
            "[mbo] 7: cp=-0.9; minsplit=5128; minbucket=1107; maxdepth=17 : y = 4.82e+07 : 602.5 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 8 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 11:23:30 PM 2025\n",
            "\n",
            "[mbo] 8: cp=-0.41; minsplit=7569; minbucket=1638; maxdepth=9 : y = 4.85e+07 : 476.3 secs : infill_ei\n",
            "\n",
            "Sat Oct 25 11:31:30 PM 2025\n",
            "\n",
            "[mbo] 9: cp=-0.646; minsplit=6824; minbucket=3408; maxdepth=13 : y = 4.81e+07 : 511.6 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 10 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 11:40:03 PM 2025\n",
            "\n",
            "[mbo] 10: cp=-0.158; minsplit=7994; minbucket=1541; maxdepth=20 : y = 4.85e+07 : 545.1 secs : infill_ei\n",
            "\n",
            "Sat Oct 25 11:49:11 PM 2025\n",
            "\n",
            "[mbo] 11: cp=-0.132; minsplit=6557; minbucket=3264; maxdepth=20 : y = 4.82e+07 : 520.9 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 12 in the file bayesian.RDATA.\n",
            "\n",
            "Sat Oct 25 11:57:55 PM 2025\n",
            "\n",
            "[mbo] 12: cp=-0.406; minsplit=658; minbucket=329; maxdepth=16 : y = 4.84e+07 : 764.9 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 13 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 12:10:42 AM 2025\n",
            "\n",
            "[mbo] 13: cp=-0.901; minsplit=508; minbucket=250; maxdepth=20 : y = 4.66e+07 : 810.7 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 14 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 12:24:16 AM 2025\n",
            "\n",
            "[mbo] 14: cp=-0.231; minsplit=18; minbucket=4; maxdepth=8 : y = 4.87e+07 : 506.5 secs : infill_ei\n",
            "\n",
            "Sun Oct 26 12:32:46 AM 2025\n",
            "\n",
            "[mbo] 15: cp=-0.702; minsplit=1326; minbucket=661; maxdepth=18 : y = 5.03e+07 : 880.0 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 16 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 12:47:28 AM 2025\n",
            "\n",
            "[mbo] 16: cp=-0.739; minsplit=4381; minbucket=20; maxdepth=19 : y = 4.64e+07 : 909.2 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 17 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 01:02:40 AM 2025\n",
            "\n",
            "[mbo] 17: cp=-0.546; minsplit=1119; minbucket=558; maxdepth=20 : y = 5.02e+07 : 733.8 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 18 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 01:14:57 AM 2025\n",
            "\n",
            "[mbo] 18: cp=-0.645; minsplit=2209; minbucket=1103; maxdepth=20 : y = 4.88e+07 : 653.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 19 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 01:25:54 AM 2025\n",
            "\n",
            "[mbo] 19: cp=-0.865; minsplit=7999; minbucket=3981; maxdepth=13 : y = 4.78e+07 : 499.1 secs : infill_ei\n",
            "\n",
            "Sun Oct 26 01:34:17 AM 2025\n",
            "\n",
            "[mbo] 20: cp=-0.609; minsplit=2; minbucket=1; maxdepth=16 : y = 3.03e+07 : 913.8 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 21 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 01:49:33 AM 2025\n",
            "\n",
            "[mbo] 21: cp=-0.531; minsplit=2985; minbucket=1489; maxdepth=20 : y = 4.73e+07 : 613.0 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 22 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 01:59:49 AM 2025\n",
            "\n",
            "[mbo] 22: cp=-0.716; minsplit=2181; minbucket=1088; maxdepth=20 : y = 4.98e+07 : 655.1 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 23 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 02:10:46 AM 2025\n",
            "\n",
            "[mbo] 23: cp=-0.671; minsplit=3666; minbucket=1822; maxdepth=20 : y = 4.76e+07 : 620.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 24 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 02:21:10 AM 2025\n",
            "\n",
            "[mbo] 24: cp=-0.422; minsplit=1290; minbucket=633; maxdepth=20 : y = 5.02e+07 : 764.1 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 25 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 02:33:56 AM 2025\n",
            "\n",
            "[mbo] 25: cp=-0.164; minsplit=5417; minbucket=1653; maxdepth=15 : y = 4.79e+07 : 609.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 26 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 02:44:10 AM 2025\n",
            "\n",
            "[mbo] 26: cp=-0.853; minsplit=7997; minbucket=1271; maxdepth=18 : y = 4.87e+07 : 611.7 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 27 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 02:54:23 AM 2025\n",
            "\n",
            "[mbo] 27: cp=-0.497; minsplit=1442; minbucket=712; maxdepth=20 : y = 5.06e+07 : 718.2 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 28 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 03:06:24 AM 2025\n",
            "\n",
            "[mbo] 28: cp=-0.232; minsplit=617; minbucket=307; maxdepth=18 : y = 4.7e+07 : 806.8 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 29 in the file bayesian.RDATA.\n",
            "\n",
            "Sun Oct 26 03:19:53 AM 2025\n",
            "\n",
            "[mbo] 29: cp=-1; minsplit=7347; minbucket=2489; maxdepth=14 : y = 4.83e+07 : 525.7 secs : infill_ei\n",
            "\n",
            "Sun Oct 26 03:28:41 AM 2025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# almaceno los resultados de la Bayesian Optimization\n",
        "# y capturo los mejores hiperparametros encontrados\n",
        "\n",
        "tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)\n",
        "\n",
        "# ordeno en forma descendente por AUC = y\n",
        "setorder(tb_bayesiana, -y)\n",
        "\n",
        "# grabo para eventualmente poder utilizarlos en OTRA corrida\n",
        "fwrite( tb_bayesiana,\n",
        "  file= \"BO_log.txt\",\n",
        "  sep= \"\\t\"\n",
        ")\n",
        "\n",
        "# los mejores hiperparámetros son los que quedaron en el registro 1 de la tabla\n",
        "PARAM$out$lgbm$mejores_hiperparametros <- tb_bayesiana[\n",
        "  1, # el primero es el de mejor AUC\n",
        "  list(cp, minsplit, minbucket, maxdepth)\n",
        "]\n",
        "\n",
        "print(PARAM$out$lgbm$mejores_hiperparametros)"
      ],
      "metadata": {
        "id": "3Yknt3FgoPM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format(Sys.time(), \"%a %b %d %X %Y\")"
      ],
      "metadata": {
        "id": "0-iLm04FMWPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nUDGTieOym3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}